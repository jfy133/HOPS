################################# 
# Read ME BY god please read me #
#################################
Example config file with all currently accepted values for a HOPS config file
Please do not use this for real analysis
###############################
#No Longer relevant for conda #
###############################
#pathToMalt=path/to/malt-run
#pathToPostProcessing=/path/PostProcessing/amps-master/postprocessing.AMPS.r
#pathToMaltExtract=/path/ro/MaltExtract1.7.jar

############
#advanced #
###########
pathToPreProcessing=path/to/RemoveHumanReads.sh
#specify the path of a preprocessing script. The current one removes reads mapping to human as runtimes in samples with high endogenous human DNA run a lot longer through the FullBacFullVir database.If you want to provide your own preprocessing script it has to produce “.fq.gz” at the end

preProcess=0
# want to run preprocessing that removes human reads before MALT. 0 = false, 1 = true


#####################
# GENEREL PARAMETERS#
#####################
#################
#MALT Parameters#
#################

#General MALT Parameters
#threadsMalt=32
#set Threads for MALT 
#maxMemoryMalt=650
#set maximum Memory for MALT in GB!!!
index=/path/to/db/full-bac-full-vir-etal-nov_2017
#path to chosen Malt DB, has to be constructed with MALT version 38 or higher

id=90.0
# set minimum percent identity only matches whose PCI value surpasses this filter will be considered. Please consider that you can set a higher Percent Identity value in MaltExtract. So you can Malt with Percent Identity 85, but run MaltExtract twice with percent identity 85 and 95, for example. Which saves hard disk space.

mpi=90.0 see malt manual

m=BlastN
#malt mode for this pipeline. the mode should always be BlastN if you plan to use the whole pipeline. But if you for whatever reason need to run BlastX, I made sure you still can do it from the framework of the pipeline.

at=SemiGlobal
#alignmentType! changing the alignment type will probably have *negative effects on the Authenticity criteria calculated* in MaltExtract! This value should only ever be changed by someone who knows what he is doing!

topMalt=1
#Only the highest scoring percentage of alignments will be considered for the LCA algorithm. Increasing this value will result in more alignments being used for the LCA algorithm, which means reads will likely be assigned to nodes higher in the taxonomy.

sup=1
#minimum number of reads to support a node for the LCA algorithm. This means at least one read is necessary to support a node.

mq=100
#maximum number of alignment that can be assinged to a read. Lowering this value will influence the taxonomic assignment and will only be worth it in a few instances.(For example in my hacked Malt version where the LCA can be disabled)

verboseMalt=1
#want verbose output for MALT. 0 = false 1 = true

memoryMode=load
#memoryMode, again only change this value when you are very experienced with the malt source code and or your computation infrastructure somehow requires a different malt mode 
(for example map can reduce the memory footprint in the ram)
additionalMaltParameters=""
#add additional parameters separated with “;”. Not all Malt parameters can be explicitly set with AMPS, but those parameters can be specified here! use with caution

########################
#MaltExtract Parameters#
########################


#General MaltExtract Parameters
threadsMaltEx=20
#set number of Threads for MaltExtract. Should never be higher than the number of input files!

maxMemoryMaltEx=300

#set maximum Memory in GB!!!! for MALTExtract.
filter=def_anc
#filter mode uses def_anc, default, ancient, scan or crawl or srna (in development) if in doubt use def_anc, which gives you results for all filtered reads in the default folder and all reads that have at least one damage lesion in their first 5 bases from either end. Default returns statistics for all reads that fullfill all other filter. Ancient requires reads in addition to have one mismatching lesion in their first 5 bases from either end. Scan just returns the number of assigned reads for all nodes without any filtering. runs very fast. Crawl was designed to replace a mapping to a specific reference. Return statistics for all references that match the input species name. For example Yersinia pestis will return statistics for all Yersinia pestis references. This probably the slowest mode.

#pathToList=/path/to/List 
#path to taxa List

resources=/Users/huebler/git/RMA6Sifter/RMASifterResurgencePrime/resources/
#path to NCBI.map and ncbi.tre files, which can be downloaded from Daniel Husons Megan github page

topMaltEx=0.01
#set top percent value for considered matches. Only the top one percent of matches will be used by default.

maxLength=0
#set a maximum read length. Only reads shorter than this value will be considered (optional). Probably only useful for def_anc or ancient.

minPIdent=0
#set minimum percent identity. Only matches with a value higher than this will be considered. (optional) You can be more strict in MaltExtract than you were in MALT

verboseMaltEx=0
# want verbose output. (0 = false 1 = true, optional ) by default false

alignment=0
# retrieve alignments for all matches that pass the filtering ( 0 = false 1 = true, optional )by default false

reads=0
# retrieve reads that fullfill filtering criteria ( 0 = false 1 = true, optional)

minComp=0
# set minimum complexity between 0.0 and 0.8. Filtering should usually only start when set to 0.6> or higher. While anything higher then 0.7 will be very strict. But maybe test for yourselves

meganSummary=0
# retrieve megan summaries. Which is a file that contains the tree structure and the number of assigned reads for each node but no alignments. This should probably be turned on if clean up is enabled so that you have some idea what the Malt Files originally contained. By default disabled.

destackingOff=0
# turn off destacking. Should only be used in files that are highly covered! (0 = false 1 = true) Otherwise any read that overlaps with any other read will be removed

dupRemOff=0
#turn off removal of duplicate reads (0 = false 1 = true, optional)

downSampOff=0
# turn off downsampling for nodes with more than 10000 assinged reads. Usually we downsample to speed up computation and 10000 assigned reads is sufficient to detect any species in Maltextract!( 0 = false 1 = true, optional)

useTopAlignment = 0;
# turn on to only use the top alignment for every statistic, except for read distribution and coverage ( 0 = false 1 = true, optional)
###########################
#PostProcessing Parameters#
###########################

threadsPost=2
#set number of threads to use in postprocessing

maxMemoryPost=10
#set Max Memory for Postprocessing

specify path to postprocessing node list
pathToList=/path/toList/default_list.txt


####################
# SLURM PARAMETERS #
####################
#######################
#MALT SLURM Parameters#
#######################


#partitionMalt=long
Set Optional partition for malt optional default will use batch

#wallTimeMalt=48:00:00
#A walltime can be set for MALT if the queue is changed from long to medium. Usually this parameter is neither set nor used. Use this only if you are sure your job can finish in 48 hours!


################################
# MaltExtraxt Slurm Parameters #
################################


#partitionMaltEx=medium
#optional set partition for MaltExtract. You can change to long in very big jobs. But I am not sure if it is advisable to change to short

#wallTimeME=48:00:00
#While it is possible to decrease the Walltime for MaltExtract this should only be done in very small jobs

#######################
#POST SLURM Parameters#
#######################
#set partition for postprocessing
#partitionPost=short
#wallTimePost =1:00:00
set walltime for postprocessing


################################
#PreProcessing Slurm Parameters#
################################

#wallTimePreProcessing =48:00:00
#Set walltime for preprocessing will only be used if partition is changed to medium or short but this only a viable option in small jobs

#partitionPreProcessing =long
#by default preproceeing will queue in long queue but you can change this

#threadsPreProcessing =32
#number of treads used for preprocessing, by default 32 threads will be used. As the shellscript itself will only require 32 cores, this parameter should only be changed when you also change the preprocessing script itself

#memoryPreProcessing =500
#change the number reserved memory for preprocessing
